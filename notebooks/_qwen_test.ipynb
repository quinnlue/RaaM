{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0d713a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from typing import Optional, Unpack\n",
    "from transformers.cache_utils import Cache, DynamicCache\n",
    "from transformers import AutoConfig\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n",
    "from transformers.utils.generic import check_model_inputs\n",
    "from transformers.modeling_layers import (\n",
    "    GradientCheckpointingLayer,\n",
    ")\n",
    "\n",
    "from typing import Optional, Union\n",
    "\n",
    "from transformers.utils import TransformersKwargs, auto_docstring, can_return_tuple\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.models.qwen2.modeling_qwen2 import (\n",
    "    Qwen2MLP,\n",
    "    Qwen2Attention,\n",
    "    Qwen2RMSNorm,\n",
    "    Qwen2RotaryEmbedding,\n",
    ")\n",
    "\n",
    "from transformers.generation import GenerationMixin\n",
    "\n",
    "from transformers.masking_utils import create_causal_mask, create_sliding_window_causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e9d4a174",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at C:\\Users\\luequ\\.cache\\huggingface\\hub\\models--Qwen--Qwen2.5-0.5B\\snapshots\\060db6499f32faf8b98477b0a26969ef7d8b9987\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\luequ\\.cache\\huggingface\\hub\\models--Qwen--Qwen2.5-0.5B\\snapshots\\060db6499f32faf8b98477b0a26969ef7d8b9987\\merges.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\luequ\\.cache\\huggingface\\hub\\models--Qwen--Qwen2.5-0.5B\\snapshots\\060db6499f32faf8b98477b0a26969ef7d8b9987\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\luequ\\.cache\\huggingface\\hub\\models--Qwen--Qwen2.5-0.5B\\snapshots\\060db6499f32faf8b98477b0a26969ef7d8b9987\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "loading configuration file config.json from cache at C:\\Users\\luequ\\.cache\\huggingface\\hub\\models--Qwen--Qwen2.5-0.5B\\snapshots\\060db6499f32faf8b98477b0a26969ef7d8b9987\\config.json\n",
      "Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4864,\n",
      "  \"layer_types\": [\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 24,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_mrope\": false,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\luequ\\.cache\\huggingface\\hub\\models--Qwen--Qwen2.5-0.5B\\snapshots\\060db6499f32faf8b98477b0a26969ef7d8b9987\\model.safetensors\n",
      "Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643\n",
      "}\n",
      "\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\luequ\\.cache\\huggingface\\hub\\models--Qwen--Qwen2.5-0.5B\\snapshots\\060db6499f32faf8b98477b0a26969ef7d8b9987\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"max_new_tokens\": 2048\n",
      "}\n",
      "\n",
      "Could not locate the custom_generate/generate.py inside Qwen/Qwen2.5-0.5B.\n",
      "loading configuration file config.json from cache at C:\\Users\\luequ\\.cache\\huggingface\\hub\\models--Qwen--Qwen2.5-0.5B\\snapshots\\060db6499f32faf8b98477b0a26969ef7d8b9987\\config.json\n",
      "Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4864,\n",
      "  \"layer_types\": [\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 24,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_mrope\": false,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 896)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4864,\n",
      "  \"layer_types\": [\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 24,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_mrope\": false,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-0.5B\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-0.5B\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"Qwen/Qwen2.5-0.5B\")\n",
    "config._attn_implementation = \"sdpa\"\n",
    "print(model)\n",
    "print(\"-\" * 100)\n",
    "print(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ffad270b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qwen2DecoderLayer(GradientCheckpointingLayer):\n",
    "    def __init__(self, config, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "\n",
    "        self.self_attn = Qwen2Attention(config=config, layer_idx=layer_idx)\n",
    "\n",
    "        self.mlp = Qwen2MLP(config)\n",
    "        self.input_layernorm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.attention_type = config.layer_types[layer_idx]\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Cache] = None,\n",
    "        use_cache: Optional[bool] = False,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n",
    "        **kwargs: Unpack[TransformersKwargs],\n",
    "    ) -> torch.Tensor:\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "        # Self Attention\n",
    "        hidden_states, _ = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            cache_position=cache_position,\n",
    "            position_embeddings=position_embeddings,\n",
    "            **kwargs,\n",
    "        )\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        # Fully Connected\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        return hidden_states\n",
    "        \n",
    "class Qwen2PreTrainedModel(PreTrainedModel):\n",
    "    config\n",
    "    base_model_prefix = \"model\"\n",
    "    supports_gradient_checkpointing = True\n",
    "    _no_split_modules = [\"Qwen2DecoderLayer\"]\n",
    "    _skip_keys_device_placement = [\"past_key_values\"]\n",
    "    _supports_flash_attn = True\n",
    "    _supports_sdpa = True\n",
    "    _supports_flex_attn = True\n",
    "\n",
    "    _can_compile_fullgraph = True\n",
    "    _supports_attention_backend = True\n",
    "    _can_record_outputs = {\n",
    "        \"hidden_states\": Qwen2DecoderLayer,\n",
    "        \"attentions\": Qwen2Attention,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "class Qwen2Model(Qwen2PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.padding_idx = config.pad_token_id\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [Qwen2DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n",
    "        )\n",
    "        self.norm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.rotary_emb = Qwen2RotaryEmbedding(config=config)\n",
    "        self.gradient_checkpointing = False\n",
    "        self.has_sliding_layers = \"sliding_attention\" in self.config.layer_types\n",
    "        self.post_init()\n",
    "\n",
    "    @check_model_inputs\n",
    "    @auto_docstring\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        **kwargs,\n",
    "    ) -> BaseModelOutputWithPast:\n",
    "        if (input_ids is None) ^ (inputs_embeds is not None):\n",
    "            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.embed_tokens(input_ids)\n",
    "\n",
    "        if use_cache and past_key_values is None:\n",
    "            past_key_values = DynamicCache(config=self.config)\n",
    "\n",
    "        if cache_position is None:\n",
    "            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n",
    "            cache_position = torch.arange(\n",
    "                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n",
    "            )\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = cache_position.unsqueeze(0)\n",
    "\n",
    "        # It may already have been prepared by e.g. `generate`\n",
    "        if not isinstance(causal_mask_mapping := attention_mask, dict):\n",
    "            # Prepare mask arguments\n",
    "            mask_kwargs = {\n",
    "                \"config\": self.config,\n",
    "                \"input_embeds\": inputs_embeds,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"cache_position\": cache_position,\n",
    "                \"past_key_values\": past_key_values,\n",
    "                \"position_ids\": position_ids,\n",
    "            }\n",
    "            # Create the masks\n",
    "            causal_mask_mapping = {\n",
    "                \"full_attention\": create_causal_mask(**mask_kwargs),\n",
    "            }\n",
    "            # The sliding window alternating layers are not always activated depending on the config\n",
    "            if self.has_sliding_layers:\n",
    "                causal_mask_mapping[\"sliding_attention\"] = create_sliding_window_causal_mask(**mask_kwargs)\n",
    "\n",
    "        hidden_states = inputs_embeds\n",
    "        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n",
    "\n",
    "        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n",
    "            hidden_states = decoder_layer(\n",
    "                hidden_states,\n",
    "                attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n",
    "                position_embeddings=position_embeddings,\n",
    "                position_ids=position_ids,\n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=use_cache,\n",
    "                cache_position=cache_position,\n",
    "                **kwargs,\n",
    "            )\n",
    "\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "        return BaseModelOutputWithPast(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=past_key_values if use_cache else None,\n",
    "        )\n",
    "\n",
    "\n",
    "class Qwen2ForCausalLM(Qwen2PreTrainedModel, GenerationMixin):\n",
    "    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n",
    "    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n",
    "    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = Qwen2Model(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    @can_return_tuple\n",
    "    @auto_docstring\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Cache] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        logits_to_keep: Union[int, torch.Tensor] = 0,\n",
    "        **kwargs: Unpack[TransformersKwargs],\n",
    "    ) -> CausalLMOutputWithPast:\n",
    "        r\"\"\"\n",
    "        Example:\n",
    "\n",
    "        ```python\n",
    "        >>> from transformers import AutoTokenizer, Qwen2ForCausalLM\n",
    "\n",
    "        >>> model = Qwen2ForCausalLM.from_pretrained(\"meta-qwen2/Qwen2-2-7b-hf\")\n",
    "        >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-qwen2/Qwen2-2-7b-hf\")\n",
    "\n",
    "        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "        >>> # Generate\n",
    "        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
    "        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n",
    "        ```\"\"\"\n",
    "        outputs: BaseModelOutputWithPast = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            cache_position=cache_position,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n",
    "        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n",
    "        logits = self.lm_head(hidden_states[:, slice_indices, :])\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "122976d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "custom_model = Qwen2ForCausalLM(config)\n",
    "custom_model.load_state_dict(model.state_dict(), strict=True)\n",
    "# Save inv_freq before conversion\n",
    "inv_freq_backup = custom_model.model.rotary_emb.inv_freq.clone()\n",
    "custom_model = custom_model.to(dtype=torch.bfloat16, device=\"cuda\")\n",
    "# Restore inv_freq to float32\n",
    "custom_model.model.rotary_emb.inv_freq = inv_freq_backup.to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9e068c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 291 parameter tensors\n"
     ]
    }
   ],
   "source": [
    "def load_hf_weights(custom_model, hf_model):\n",
    "    \"\"\"\n",
    "    Load weights from a HuggingFace model into the custom model.\n",
    "    \n",
    "    Args:\n",
    "        custom_model: Custom Qwen2ForCausalLM instance\n",
    "        hf_model: HuggingFace AutoModelForCausalLM instance\n",
    "    \n",
    "    Returns:\n",
    "        custom_model with loaded weights\n",
    "    \"\"\"\n",
    "    hf_state = hf_model.state_dict()\n",
    "    \n",
    "    # Load state dict with strict=True to ensure exact match\n",
    "    custom_model.load_state_dict(hf_state, strict=True)\n",
    "    \n",
    "    print(f\"Successfully loaded {len(hf_state)} parameter tensors\")\n",
    "    return custom_model\n",
    "\n",
    "# Load weights and convert to bfloat16\n",
    "custom_model = load_hf_weights(custom_model, model)\n",
    "\n",
    "# Save inv_freq before dtype conversion\n",
    "inv_freq = custom_model.model.rotary_emb.inv_freq.clone()\n",
    "\n",
    "# Convert model to bfloat16\n",
    "custom_model = custom_model.to(dtype=torch.bfloat16, device=\"cuda\")\n",
    "\n",
    "# Restore inv_freq to float32 (critical for numerical accuracy)\n",
    "custom_model.model.rotary_emb.inv_freq = inv_freq.to(device=\"cuda\", dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c5e6ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "04791712",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Prove that there multiplication is a continuous map using a delta epsilon argument\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs.input_ids = inputs.input_ids.to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9f5a679e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "generate_ids = custom_model.generate(inputs.input_ids, max_new_tokens=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7e0fd180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Prove that there multiplication is a continuous map using a delta epsilon argument. I am trying to prove that the multiplication map $m: \\\\mathbb{R} \\\\times \\\\mathbb{R} \\\\to \\\\mathbb{R}$ is continuous. I am trying to use a delta epsilon argument, but I am not sure how to do it. I know that the multiplication map is continuous if and only if it is continuous at every point in $\\\\mathbb{R} \\\\times \\\\mathbb{R}$, but I am not sure how to use this to prove that it is continuous at every point in $\\\\mathbb{R} \\\\times \\\\'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(generate_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d43c17b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEST: Forward Pass Equivalence\n",
      "============================================================\n",
      "✓ Prompt: 'Hello world...'\n",
      "   Max diff: 0.00e+00, Mean diff: 0.00e+00\n",
      "✓ Prompt: 'The quick brown fox jumps over the lazy ...'\n",
      "   Max diff: 0.00e+00, Mean diff: 0.00e+00\n",
      "✓ Prompt: 'In mathematics, a prime number is...'\n",
      "   Max diff: 0.00e+00, Mean diff: 0.00e+00\n",
      "✓ Prompt: 'def fibonacci(n):...'\n",
      "   Max diff: 0.00e+00, Mean diff: 0.00e+00\n",
      "\n",
      "============================================================\n",
      "✓ Forward pass equivalence test PASSED!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# === Forward Pass Equivalence Test ===\n",
    "\n",
    "def test_forward_equivalence(custom_model, hf_model, tokenizer, test_prompts=None):\n",
    "    \"\"\"\n",
    "    Test that both models produce identical outputs for the same inputs.\n",
    "    \"\"\"\n",
    "    if test_prompts is None:\n",
    "        test_prompts = [\n",
    "            \"Hello world\",\n",
    "            \"The quick brown fox jumps over the lazy dog.\",\n",
    "            \"In mathematics, a prime number is\",\n",
    "            \"def fibonacci(n):\",\n",
    "        ]\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"TEST: Forward Pass Equivalence\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    custom_model.eval()\n",
    "    hf_model.eval()\n",
    "    \n",
    "    all_passed = True\n",
    "    \n",
    "    for prompt in test_prompts:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        input_ids = inputs.input_ids.to(hf_model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # HuggingFace model output\n",
    "            hf_output = hf_model(input_ids)\n",
    "            hf_logits = hf_output.logits\n",
    "            \n",
    "            # Custom model output\n",
    "            custom_output = custom_model(input_ids)\n",
    "            custom_logits = custom_output[\"logits\"]\n",
    "        \n",
    "        # Compare logits\n",
    "        max_diff = (hf_logits.float() - custom_logits.float()).abs().max().item()\n",
    "        mean_diff = (hf_logits.float() - custom_logits.float()).abs().mean().item()\n",
    "        \n",
    "        # Use a reasonable tolerance for bfloat16\n",
    "        is_close = torch.allclose(hf_logits.float(), custom_logits.float(), atol=1e-4, rtol=1e-3)\n",
    "        \n",
    "        status = \"✓\" if is_close else \"✗\"\n",
    "        print(f\"{status} Prompt: '{prompt[:40]}...'\")\n",
    "        print(f\"   Max diff: {max_diff:.2e}, Mean diff: {mean_diff:.2e}\")\n",
    "        \n",
    "        if not is_close:\n",
    "            all_passed = False\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    if all_passed:\n",
    "        print(\"✓ Forward pass equivalence test PASSED!\")\n",
    "    else:\n",
    "        print(\"✗ Forward pass equivalence test FAILED!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return all_passed, hf_logits, custom_logits\n",
    "\n",
    "# Run forward pass test\n",
    "_, hf_logits, custom_logits = test_forward_equivalence(custom_model, model, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428be4e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff620c5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c212aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa217733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Shape |  Time (µs)\n",
      "--------------------------\n",
      "128x128x128 |      57.02\n",
      "127x127x127 |     101.51\n",
      "256x256x256 |      39.78\n",
      "255x255x255 |      32.00\n",
      "4096x4096x4096 |    6473.58\n",
      "4095x4095x4095 |    7260.30\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba2b0db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
